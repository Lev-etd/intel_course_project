{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFEW Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=r'D:\\Users\\amira\\Documents\\datasets\\emotions\\AudioVideo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir(dirname):\n",
    "    resdir=os.path.join(DATA_DIR,'frames', 'Val_AFEW')\n",
    "    d = os.path.normpath(os.path.join(DATA_DIR, 'Val_AFEW', dirname))\n",
    "    for filename in tqdm(os.listdir(d)):\n",
    "        if os.path.isdir(os.path.normpath(os.path.join(d,filename))):\n",
    "            videofile=None\n",
    "            for fn in os.listdir(os.path.normpath(os.path.join(d,filename))):\n",
    "                videofile=fn\n",
    "            if videofile is None:\n",
    "                continue\n",
    "            filename=os.path.normpath(os.path.join(filename,videofile))\n",
    "        fn, ext = os.path.splitext(os.path.basename(filename))\n",
    "        outdir=os.path.normpath(os.path.join(resdir,dirname,fn))\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "        command = \"ffmpeg -r 1 -i \"+os.path.join(d,filename) + \" -r 1 \"+outdir+\"/%05d.png\"\n",
    "        command = os.path.normpath(command)\n",
    "        print(command)\n",
    "        os.system(command=command)\n",
    "\n",
    "\n",
    "for emotion_name in os.listdir(os.path.normpath(os.path.join(DATA_DIR, 'Val_AFEW'))):\n",
    "    process_dir(emotion_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get one face from frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D:\\\\Users\\\\amira\\\\emotion-recognition\\\\code\\\\mtcnn.pb': ''}\n"
     ]
    }
   ],
   "source": [
    "from facial_analysis import FacialImageProcessing\n",
    "imgProcessing=FacialImageProcessing(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='D:/Users/amira/Documents/datasets/emotions/AudioVideo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (224,224)\n",
    "def save_faces(source_path,save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for folder in tqdm(os.listdir(source_path)):\n",
    "        #print(folder)\n",
    "        if not os.path.exists(os.path.join(save_path, folder)):\n",
    "            os.mkdir(os.path.join(save_path, folder))\n",
    "\n",
    "            for image in os.listdir(os.path.join(source_path, folder)):\n",
    "                filename = os.path.join(source_path, folder, image)\n",
    "                frame_bgr = cv2.imread(filename)\n",
    "                frame = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "                bounding_boxes, _ = imgProcessing.detect_faces(frame)\n",
    "\n",
    "                if len(bounding_boxes)==0:\n",
    "                    print('No faces found for ',filename)\n",
    "                    face_img = frame_bgr\n",
    "                    faceFound='noface'\n",
    "                else:\n",
    "                    if len(bounding_boxes)>1:\n",
    "                        print('Too many faces (',len(bounding_boxes),') found for ',filename)\n",
    "                        bounding_boxes=bounding_boxes[:1]\n",
    "\n",
    "                    b=[int(bi) for bi in bounding_boxes[0]]\n",
    "                    x1,y1,x2,y2=b[0:4]\n",
    "                    face_img=frame_bgr[y1:y2,x1:x2,:]\n",
    "\n",
    "                    if np.prod(face_img.shape)==0:\n",
    "                        print('Empty face ',b,' found for ',filename)\n",
    "                        continue\n",
    "                \n",
    "                    faceFound=''\n",
    "\n",
    "                root,ext=os.path.splitext(image)\n",
    "                cv2.imwrite(os.path.join(save_path, folder, root+faceFound+ext), face_img) \n",
    "        else:\n",
    "            print(folder)\n",
    "\n",
    "for emotion_name in os.listdir(os.path.normpath(os.path.join(DATA_DIR, 'frames/Train_AFEW'))):        \n",
    "    save_faces(os.path.join(DATA_DIR,'frames/Train_AFEW', emotion_name), os.path.join(DATA_DIR,'faces/Train_AFEW', emotion_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get multiple faces from frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'D:\\\\Users\\\\amira\\\\emotion-recognition\\\\code\\\\mtcnn.pb': ''}\n"
     ]
    }
   ],
   "source": [
    "from facial_analysis import FacialImageProcessing\n",
    "imgProcessing=FacialImageProcessing(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='D:/Users/amira/Documents/datasets/emotions/AudioVideo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (224,224)\n",
    "def save_faces(source_path,save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for folder in tqdm(os.listdir(source_path)):\n",
    "        #print(folder)\n",
    "        if not os.path.exists(os.path.join(save_path, folder)):\n",
    "            os.mkdir(os.path.join(save_path, folder))\n",
    "\n",
    "            for image in os.listdir(os.path.join(source_path, folder)):\n",
    "                filename = os.path.join(source_path, folder, image)\n",
    "                frame_bgr = cv2.imread(filename)\n",
    "                frame = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "                bounding_boxes, _ = imgProcessing.detect_faces(frame)\n",
    "\n",
    "                if len(bounding_boxes)==0:\n",
    "                    print('No faces found for ',filename)\n",
    "                    face_img = frame_bgr\n",
    "                    faceFound='noface'\n",
    "                else:\n",
    "                    print('(',len(bounding_boxes),') faces found for ',filename)\n",
    "                    for i in range(len(bounding_boxes)):\n",
    "                        bounding_box=bounding_boxes[i]\n",
    "\n",
    "                        b=[int(bi) for bi in bounding_box]\n",
    "                        x1,y1,x2,y2=b[0:4]\n",
    "                        face_img=frame_bgr[y1:y2,x1:x2,:]\n",
    "\n",
    "                        if np.prod(face_img.shape)==0:\n",
    "                            print('Empty face ',b,' found for ',filename)\n",
    "                            continue\n",
    "                    \n",
    "                        faceFound='_'+str(i)\n",
    "\n",
    "                #face_img=cv2.resize(face_img,INPUT_SIZE)\n",
    "                root,ext=os.path.splitext(image)\n",
    "                cv2.imwrite(os.path.join(save_path, folder, root+faceFound+ext), face_img) \n",
    "        else:\n",
    "            print(folder)\n",
    "\n",
    "for emotion_name in os.listdir(os.path.normpath(os.path.join(DATA_DIR, 'frames/Train_AFEW'))):        \n",
    "    save_faces(os.path.join(DATA_DIR,'frames/Train_AFEW', emotion_name), os.path.join(DATA_DIR,'faces/Train_AFEW', emotion_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get OpenFace features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='D:/Users/amira/Documents/datasets/emotions/AudioVideo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Users/amira/Documents/datasets/emotions/AudioVideo\\frames/Train_AFEW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [05:41<00:00,  2.57s/it]\n",
      "100%|██████████| 74/74 [03:08<00:00,  2.55s/it]\n",
      "100%|██████████| 81/81 [03:06<00:00,  2.30s/it]\n",
      "100%|██████████| 150/150 [06:06<00:00,  2.44s/it]\n",
      "100%|██████████| 144/144 [05:42<00:00,  2.38s/it]\n",
      "100%|██████████| 117/117 [04:45<00:00,  2.44s/it]\n",
      "100%|██████████| 74/74 [02:55<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Users/amira/Documents/datasets/emotions/AudioVideo\\frames/Val_AFEW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [02:46<00:00,  2.60s/it]\n",
      "100%|██████████| 40/40 [01:38<00:00,  2.47s/it]\n",
      "100%|██████████| 46/46 [01:43<00:00,  2.25s/it]\n",
      "100%|██████████| 63/63 [02:29<00:00,  2.38s/it]\n",
      "100%|██████████| 63/63 [02:23<00:00,  2.28s/it]\n",
      "100%|██████████| 61/61 [02:15<00:00,  2.22s/it]\n",
      "100%|██████████| 46/46 [01:31<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_openface_features(dirname,outdir):\n",
    "    print(dirname)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    for emotion_name in os.listdir(dirname):\n",
    "        for video_name in tqdm(os.listdir(os.path.join(dirname,emotion_name))):\n",
    "            command=r'D:\\Users\\amira\\Documents\\OpenFace_2.2.0_win_x64\\FeatureExtraction.exe -pose -aus -gaze -out_dir '+outdir+emotion_name\n",
    "            frames_dir=os.path.join(dirname,emotion_name,video_name)\n",
    "            if os.path.isdir(frames_dir):\n",
    "                command+=' -fdir '+frames_dir\n",
    "                os.system(command=command)\n",
    "    \n",
    "\n",
    "cur_dir=os.getcwd()\n",
    "extract_openface_features(os.path.join(DATA_DIR,'frames/Train_AFEW'),os.path.join(DATA_DIR,'openface/Train_AFEW/'))\n",
    "extract_openface_features(os.path.join(DATA_DIR,'frames/Val_AFEW'),os.path.join(DATA_DIR,'openface/Val_AFEW/'))\n",
    "os.chdir(cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5b0939b96e20240b6688bffb0ff6dfbaab1e456544936bbdb82861744a570ee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
